<html><head>
<title>Automatic Music Affect Recognition</title>
<meta http-equiv="content-type" content="text/html; charset=iso-8859-1">
<!--<base href=".">-->
</head>

<body>

<table width="100%"><tbody>
	<hr>
<tr>
<td>&nbsp;</td>
<td bgcolor="cccc99", align="left" valign="top" width="23%">
<hr>
<a href="https://liacs.leidenuniv.nl/~bakkerem2/api/">Audio Processing Indexing Course LIACS 2021</a>
<hr>
<a href="https://github.com/TheBeast762/TheBeast762.github.io">
   Project GitHub source code</a> 
<hr>
<a href="https://iopscience.iop.org/article/10.1088/1757-899X/482/1/012019/pdf">Publication taken as inspiration (2019)</a>
<hr>

</td>


<td>&nbsp;</td>
<td>&nbsp;</td>
<td valign="TOP">
<br>
<center><h1>Automatic Valence and Arousal Recognition in Music</h1>
<h3>Damian Domela s1853767</h3>
<h3>Leiden Institute of Advanced Computer Science, May 2021</h3>
</center>
	<hr>

<h3>Context</h3>
<p>The aim of this project was to construct a model which annotates large digital music databases. The target output of this model is Russell's model of Affect, in which x-axis corresponds to 'Valence' (the extent to which an emotion is positive or negative) while the y-axis corresponds to 'Arousal' (the intensity of the associated emotion). The actual structure of the model ensemble is based on Tan et al. [2].</p>
<center>
<figure>
  <img width=250, src="https://raw.githubusercontent.com/TheBeast762/TheBeast762.github.io/main/russell.png">
  <figcaption>Russell's model of Affect [1]</figcaption>
</figure>
</center>

<h3>Bimodal Dataset</h3>
<p>This project features a bimodal dataset containing 30 second audio clips with their corresponding lyric text files, this dataset was taken from Panda et al. [3] The original dataset contained 133 bimodal data points, but there was a missing overlap between audio and lyric files. The missing lyric files were manually extended, resulting in a bimodal dataset of 162 data points. Unfortunately, this enlarged dataset is still very small. It isn't easily extended though, as its annotation was done by 39 human annotators, who reached an inter-annotator agreement of ~0.85.</p>
<center>
<figure>
  <img width=250, src="https://raw.githubusercontent.com/TheBeast762/TheBeast762.github.io/main/class_distr.png">
  <figcaption>Class distribution for improved bimodal dataset</figcaption>
</figure>
</center>

<h3>Feature Extraction</h3>
<p>Audio features were extracted using Giannakopoulos' open-source library <a href="https://github.com/tyiannak/pyAudioAnalysis">pyAudioAnalysis</a>. The following feature set selection was made, based on the research of Grekow et al. [4]:</p>
<center>
<table border = "1">
         <tr>
            <td align="center", colspan="6"><b>Audio Features</b></td>
         </tr>
         
         <tr>
            <td>Energy</td>
            <td>Entropy of Energy</td>
            <td>Spectral Entropy</td>
            <td>Beats per minute</td>
            <td>Spectral Roll-off</td>
            <td>Spectral Flux</td>
         </tr>
         <tr>
         	<td>StdDev Energy</td>
            <td>StdDev Entropy of Energy</td>
            <td>StdDev Spectral Entropy</td>
            <td>StdDev Beats per minute</td>
            <td>StdDev Spectral Roll-off</td>
            <td>StdDev Spectral Flux</td>
         </tr>
      </table>
</center>
<p>The raw lyric files were pre-processed by removing stop words/punctuation and lemmatizing the words to their lemma. These pre-processed files were later used to match with a valence dictionary, in which each word has an associated valence score. This method was based on the method by Warriner et al.[5]</p>

<h3>Model Ensemble</h3>
<p>A Naive Bayes model was trained given the individual valence scores for each word, the overall valence class of the lyric file was determined by considering whether each word has negative or positive valence and giving preference to the highest probability class.</p>
<p>A Support Vector Machine was trained on all previously mentioned audio features, with the correct labels being based on the bimodal dataset. Creating a linear hyperplane between both classes is most likely impossible, resulting in this model being guaranteed to be imperfect.</p>
<p>These two models are combined in an ensemble, which outputs a class for both Valence and Arousal. These predictions can be mapped as coordinates to the 4 quadrants of the Russell's model.</p>

<h3>Experiment</h3>
<p>Seeing as the dataset is very small, k-fold cross-validation can produce skewed results due to extreme outliers (which are expected due to Naive Bayes being sensitive to unbalanced training set class distributions). Instead, preference is given to repeated random sub-sampling, allowing for a large amount of iterations of a 10% test set and 90% training set split, averaging out to more trustworthy performance results.</p>

<center>
<figure>
  <img width=300, src="https://raw.githubusercontent.com/TheBeast762/TheBeast762.github.io/main/randomsub-sampling.png">
  <figcaption>Random sub-sampling method</figcaption>
</figure>
</center>

<h3>Results</h3>
<center>
<figure>
  <img width=600, src="https://raw.githubusercontent.com/TheBeast762/TheBeast762.github.io/main/accuracies.png">
  <figcaption>Accuracy(%) whisker plots</figcaption>
</figure>
</center>
<p>Although there is a significant amount of variance in the results, the mean accuracies of the Naive Bayes model, the SVM and the ensemble are 78%, 78% and 62% respectively. This is a bit lower than the state-of-the-art performance in this field, as Catharin et al. [6] reached an accuracy of 73%.</p>
<center>
<figure>
  <img width=300, src="https://raw.githubusercontent.com/TheBeast762/TheBeast762.github.io/main/visualTool.png">
  <figcaption>Visual Tool to assess performance</figcaption>
</figure>
</center>
<p>Finally, a simple visual tool was developed in order to somewhat personify the predictions of the model ensemble. At times the incorrectly predicted Valence or Arousal classes are defensible. For example, if a Valence prediction is incorrect, the lyrics can bear both positive and negative Valence, depending on the interpretation. This hints at this project still being applicable for recommendation of songs based on the user's listening history.</p>

<h3>Conclusion</h3>
<p>With an overall accuracy of 62%, this project failed to break the state-of-the-art performance threshold of competing algorithms, but might still be usable for song recommendation. Still, developing a simple reconstruction of [2] was a fun and educational way to learn about audio feature extraction and modelling.</p>

<hr>
<h4>References</h4>
<p>[1] Russell, James. (1980). A Circumplex Model of Affect. Journal of Personality and Social Psychology. 39. 1161-1178. 10.1037/h0077714</p>
<a href="https://iopscience.iop.org/article/10.1088/1757-899X/482/1/012019/pdf">[2] Automatic music mood recognition using Russell's twodimensional valence-arousal space from audio and lyrical data as classified using SVM and Naive Bayes - K R Tan, M L Villarino, C Maderazo (2019)</a>
<p>[3] Bi-modal music emotion recognition: Novel lyrical features and dataset - R. Malheiro, R. Panda, Paulo Gomes, R. Paiva (2016)</p>
<p>[4] Audio features dedicated to the detection and tracking of arousal and valence in musical compositions - J Grekow (2018)</p>
<p>[5] Norms of valence, arousal, and dominance for 13,915 English lemmas - Warriner A B, Kuperman V and Brysbaert M (2013)</p>
<p>[6] Multimodal Classification of Emotions in Latin Music - L. G. Catharin and Rafael P. Ribeiro and C. N. Silla and Yandre M. G. Costa and V. D. Feltrim (2020)</p>
<p></p>

</td></tr>

<tr><td colspan="6" bgcolor="cccc99">
<font size="2">github.io website extension of corresponding github framework
</font></td></tr></tbody></table>





</body></html>